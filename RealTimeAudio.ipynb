{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T03:44:41.670706Z",
     "start_time": "2024-08-22T03:44:41.660137Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definir la clase EmotionRecognitionModel antes de cargar el modelo\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = out[:, -1, :]  # Tomar la salida del último timestep\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Cargar el modelo completo\n",
    "model = torch.load('modelo_emociones_lstm_completo.pth')\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/hj1tkdtn4xd2lk842kp9xy7r0000gn/T/ipykernel_6684/4264266241.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('modelo_emociones_lstm_completo.pth')\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Asegúrate de que el modelo esté en modo de evaluación y en el dispositivo correcto\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "\n",
    "# Configuración de la secuencia de tiempo (timesteps)\n",
    "n_timesteps = 101  # Debe coincidir con lo que usaste para entrenar el modelo\n",
    "n_features = 38  # Asegúrate de que esto coincida con el número de características por timestep usado en el entrenamiento\n",
    "\n",
    "# Inicializar un buffer para mantener las secuencias de características en tiempo real\n",
    "sequence_buffer = deque(maxlen=n_timesteps)\n",
    "\n",
    "# Inicializar la emoción previa para detección de cambios\n",
    "previous_emotion = None\n",
    "\n",
    "# Simulación de obtención de características en tiempo real (ajusta para que devuelva 38 características)\n",
    "def get_real_time_features():\n",
    "    # Esta función debería devolver un vector de características de longitud `n_features` (38 en este caso)\n",
    "    # Aquí simplemente devolvemos datos aleatorios como ejemplo\n",
    "    return np.random.rand(n_features)\n",
    "\n",
    "# Ejemplo de bucle de predicción en tiempo real\n",
    "while True:\n",
    "    # Obtener nuevas características en tiempo real\n",
    "    new_features = get_real_time_features()\n",
    "\n",
    "    # Añadir las nuevas características al buffer\n",
    "    sequence_buffer.append(new_features)\n",
    "\n",
    "    # Verificar si hemos llenado el buffer con suficientes timesteps\n",
    "    if len(sequence_buffer) == n_timesteps:\n",
    "        # Convertir el buffer a un tensor y hacer la predicción\n",
    "        sequence_array = np.array(sequence_buffer)\n",
    "        input_tensor = torch.tensor(sequence_array, dtype=torch.float32).unsqueeze(0).to(device)  # Añadir dimensión de batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            current_emotion = predicted.item()\n",
    "\n",
    "        # Detectar cambio de emoción\n",
    "        if current_emotion != previous_emotion:\n",
    "            print(f'Emoción cambiada: {current_emotion}')\n",
    "            previous_emotion = current_emotion\n",
    "            # Aquí podrías agregar lógica para mostrar la nueva emoción, guardarla, etc.\n",
    "\n",
    "    # Simulación de esperar un poco antes de la siguiente iteración (ej. 100ms)\n",
    "    # Para tiempos reales, esta parte dependería de cómo obtienes las características en tiempo real\n",
    "    # time.sleep(0.1)\n"
   ],
   "id": "1e173fada68e6810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:46:04.091979Z",
     "start_time": "2024-08-22T03:45:59.468735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, Counter\n",
    "\n",
    "# Asegúrate de que el modelo esté en modo de evaluación y en el dispositivo correcto\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "\n",
    "# Configuración de la secuencia de tiempo (timesteps)\n",
    "n_timesteps = 101\n",
    "n_features = 38\n",
    "\n",
    "# Inicializar un buffer para mantener las secuencias de características en tiempo real\n",
    "sequence_buffer = deque(maxlen=n_timesteps)\n",
    "\n",
    "# Ventana deslizante para suavizar predicciones\n",
    "prediction_window = deque(maxlen=5)  # Ventana de las últimas 5 predicciones\n",
    "\n",
    "# Inicializar la emoción previa para detección de cambios\n",
    "previous_emotion = None\n",
    "\n",
    "# Simulación de obtención de características en tiempo real\n",
    "def get_real_time_features():\n",
    "    return np.random.rand(n_features)\n",
    "\n",
    "# Bucle de predicción en tiempo real\n",
    "while True:\n",
    "    new_features = get_real_time_features()\n",
    "    sequence_buffer.append(new_features)\n",
    "\n",
    "    if len(sequence_buffer) == n_timesteps:\n",
    "        sequence_array = np.array(sequence_buffer)\n",
    "        input_tensor = torch.tensor(sequence_array, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            current_emotion = predicted.item()\n",
    "\n",
    "        # Añadir la nueva predicción a la ventana deslizante\n",
    "        prediction_window.append(current_emotion)\n",
    "\n",
    "        # Solo cambiar la emoción si la mayoría de las últimas predicciones son consistentes\n",
    "        most_common_emotion, count = Counter(prediction_window).most_common(1)[0]\n",
    "        if most_common_emotion != previous_emotion and count > 2:  # Cambiar solo si hay al menos 3 predicciones consistentes\n",
    "            print(f'Emoción cambiada: {most_common_emotion}')\n",
    "            previous_emotion = most_common_emotion\n",
    "\n",
    "    # time.sleep(0.1)  # Ajusta según la velocidad de adquisición de características en tiempo real\n"
   ],
   "id": "15dbe631200c6692",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n",
      "Emoción cambiada: 6\n",
      "Emoción cambiada: 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 36\u001B[0m\n\u001B[1;32m     33\u001B[0m input_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(sequence_array, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 36\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(input_tensor)\n\u001B[1;32m     37\u001B[0m     _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(output, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     38\u001B[0m     current_emotion \u001B[38;5;241m=\u001B[39m predicted\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 15\u001B[0m, in \u001B[0;36mEmotionRecognitionModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     12\u001B[0m h_0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     13\u001B[0m c_0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 15\u001B[0m out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm(x, (h_0, c_0))\n\u001B[1;32m     16\u001B[0m out \u001B[38;5;241m=\u001B[39m out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]  \u001B[38;5;66;03m# Tomar la salida del último timestep\u001B[39;00m\n\u001B[1;32m     17\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(out)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:917\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    914\u001B[0m         hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 917\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    918\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    920\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[1;32m    921\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Cargar el modelo completo\n",
    "model = torch.load('modelo_emociones_lstm_completo.pth')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()  # Establecer el modelo en modo de evaluación\n",
    "\n",
    "# Configuración de la secuencia de tiempo (timesteps)\n",
    "n_timesteps = 101  # Asegúrate de que coincida con lo que espera tu modelo\n",
    "n_features = model.fc.in_features  # Esto debería coincidir con el número de características por timestep\n",
    "\n",
    "# Inicializar un buffer para mantener las secuencias de características de tiempo real\n",
    "sequence_buffer = deque(maxlen=n_timesteps)\n",
    "\n",
    "# Suponiendo que tienes un flujo de características en tiempo real (aquí usaremos una función ficticia)\n",
    "def get_real_time_features():\n",
    "    # Esta función debería devolver un vector de características de longitud `n_features`\n",
    "    # Aquí simplemente devolveremos datos aleatorios como ejemplo\n",
    "    return np.random.rand(n_features)\n",
    "\n",
    "# Ejemplo de bucle de predicción en tiempo real\n",
    "while True:\n",
    "    # Obtener nuevas características en tiempo real\n",
    "    new_features = get_real_time_features()\n",
    "\n",
    "    # Añadir las nuevas características al buffer\n",
    "    sequence_buffer.append(new_features)\n",
    "\n",
    "    # Verificar si hemos llenado el buffer con suficientes timesteps\n",
    "    if len(sequence_buffer) == n_timesteps:\n",
    "        # Convertir el buffer a un tensor y hacer la predicción\n",
    "        sequence_array = np.array(sequence_buffer)\n",
    "        input_tensor = torch.tensor(sequence_array, dtype=torch.float32).unsqueeze(0).to(device)  # Añadir dimensión de batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_emotion = predicted.item()\n",
    "            print(f'Emoción predicha: {predicted_emotion}')\n",
    "\n",
    "        # Aquí podrías agregar una lógica para mostrar la emoción en pantalla, guardarla, etc.\n",
    "\n",
    "    # Simulación de esperar un poco antes de la siguiente iteración (ej. 100ms)\n",
    "    # Para tiempos reales, esta parte dependería de cómo obtienes las características en tiempo real\n",
    "    # time.sleep(0.1)\n"
   ],
   "id": "8e77cd8f976d1d71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

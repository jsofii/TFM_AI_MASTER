{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T03:14:22.665440Z",
     "start_time": "2024-08-22T03:14:22.463130Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# Inicializar el modelo\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 7)\n",
    "\n",
    "# Cargar los pesos del modelo en la CPU\n",
    "state_dict = torch.load('modelo_emociones_imagen.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# Cargar los pesos en el modelo\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Asegurarse de que el modelo esté en la CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Ahora el modelo está listo para usarse en la CPU\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/hj1tkdtn4xd2lk842kp9xy7r0000gn/T/ipykernel_5972/2622852045.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('modelo_emociones_imagen.pth', map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:14:24.910835Z",
     "start_time": "2024-08-22T03:14:24.907360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define las transformaciones necesarias para las imágenes\n",
    "from torchvision import models, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Mapeo de índices a nombres de emociones (esto debería coincidir con las etiquetas de tu dataset)\n",
    "emotion_map = {0: 'Enojo', 1: 'Disgusto', 2: 'Miedo', 3: 'Felicidad', 4: 'Tristeza', 5: 'Sorpresa', 6: 'Neutral'}\n"
   ],
   "id": "fb33662a9b022f87",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:16:52.184986Z",
     "start_time": "2024-08-22T03:16:32.147912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Crear una carpeta para guardar las imágenes si no existe\n",
    "output_dir = \"captured_images\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Captura de video desde la cámara\n",
    "cap = cv2.VideoCapture(0)  # 0 para la cámara por defecto\n",
    "\n",
    "image_count = 0  # Contador para guardar imágenes con nombres únicos\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Captura el frame actual\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Guardar la imagen original capturada\n",
    "    cv2.imwrite(f\"{output_dir}/original_{image_count}.jpg\", frame)\n",
    "\n",
    "    # Preprocesa la imagen\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # Convertir a PIL Image\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # Aplicar transformaciones y añadir batch dimension\n",
    "\n",
    "    # Guardar la imagen preprocesada (escala de grises, redimensionada)\n",
    "    img_gray = img_tensor.squeeze().cpu().numpy()  # Quitar la dimensión extra y convertir a numpy\n",
    "    img_gray = (img_gray * 255).astype(\"uint8\")  # Desnormalizar para guardar como imagen\n",
    "    cv2.imwrite(f\"{output_dir}/preprocessed_{image_count}.jpg\", img_gray)\n",
    "\n",
    "    # Realiza la predicción\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        emotion = emotion_map[predicted.item()]\n",
    "\n",
    "    # Muestra la predicción en el frame\n",
    "    cv2.putText(frame, f'Emocion: {emotion}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Muestra el frame con la predicción\n",
    "    cv2.imshow('Reconocimiento de Emociones en Tiempo Real', frame)\n",
    "\n",
    "    # Incrementar el contador de imágenes\n",
    "    image_count += 1\n",
    "\n",
    "    # Presiona 'q' para salir del bucle\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Libera los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "ce2c0a2c8ab17f21",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m image_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Contador para guardar imágenes con nombres únicos\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 14\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()  \u001B[38;5;66;03m# Captura el frame actual\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:32:42.735943Z",
     "start_time": "2024-08-22T03:32:14.519858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 7)\n",
    "\n",
    "# Cargar los pesos del modelo en la CPU\n",
    "state_dict = torch.load('modelo_emociones_imagen.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Asegurarse de que el modelo esté en modo de evaluación y en la CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()  # Asegurarse de que el modelo está en modo de evaluación\n",
    "\n",
    "# Define las transformaciones necesarias, directamente redimensionando a 48x48\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),  # Redimensionar directamente a 48x48 para la predicción\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Mapeo de índices a nombres de emociones\n",
    "emotion_map = {0: 'Enojo', 1: 'Disgusto', 2: 'Miedo', 3: 'Felicidad', 4: 'Tristeza', 5: 'Sorpresa', 6: 'Neutral'}\n",
    "\n",
    "# Crear una carpeta para guardar las imágenes si no existe\n",
    "output_dir = \"captured_images\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Captura de video desde la cámara\n",
    "cap = cv2.VideoCapture(0)  # 0 para la cámara por defecto\n",
    "\n",
    "image_count = 0  # Contador para guardar imágenes con nombres únicos\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Captura el frame actual\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Guardar la imagen original capturada\n",
    "    cv2.imwrite(f\"{output_dir}/original_{image_count}.jpg\", frame)\n",
    "\n",
    "    # Convertir la imagen a escala de grises, pero no reducir la resolución aún\n",
    "    img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Redimensionar la imagen a 48x48 directamente con una interpolación adecuada (INTER_AREA)\n",
    "    img_gray_resized = cv2.resize(img_gray, (48, 48), interpolation=cv2.INTER_AREA)  # Evitar pérdida de calidad\n",
    "\n",
    "    # Guardar la imagen redimensionada a 48x48 para inspección\n",
    "    cv2.imwrite(f\"{output_dir}/preprocessed_{image_count}.jpg\", img_gray_resized)\n",
    "\n",
    "    # Convertir la imagen redimensionada a tensor para el modelo\n",
    "    img_tensor = transform(Image.fromarray(img_gray_resized)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Realiza la predicción\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        emotion = emotion_map[predicted.item()]\n",
    "\n",
    "    # Muestra la predicción en el frame\n",
    "    cv2.putText(frame, f'Emocion: {emotion}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Muestra el frame con la predicción\n",
    "    cv2.imshow('Reconocimiento de Emociones en Tiempo Real', frame)\n",
    "\n",
    "    # Incrementar el contador de imágenes\n",
    "    image_count += 1\n",
    "\n",
    "    # Presiona 'q' para salir del bucle\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Libera los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "c5bc86063c68aa4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/hj1tkdtn4xd2lk842kp9xy7r0000gn/T/ipykernel_5972/2514918474.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('modelo_emociones_imagen.pth', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m image_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Contador para guardar imágenes con nombres únicos\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 45\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()  \u001B[38;5;66;03m# Captura el frame actual\u001B[39;00m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:35:30.447786Z",
     "start_time": "2024-08-22T03:35:05.571147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 7)\n",
    "\n",
    "# Cargar los pesos del modelo en la CPU\n",
    "state_dict = torch.load('modelo_emociones_imagen.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Asegurarse de que el modelo esté en modo de evaluación y en la CPU\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()  # Asegurarse de que el modelo está en modo de evaluación\n",
    "\n",
    "# Define las transformaciones necesarias\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Mapeo de índices a nombres de emociones\n",
    "emotion_map = {0: 'Enojo', 1: 'Disgusto', 2: 'Miedo', 3: 'Felicidad', 4: 'Tristeza', 5: 'Sorpresa', 6: 'Neutral'}\n",
    "\n",
    "# Crear una carpeta para guardar las imágenes si no existe\n",
    "output_dir = \"captured_images\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Captura de video desde la cámara\n",
    "cap = cv2.VideoCapture(0)  # 0 para la cámara por defecto\n",
    "\n",
    "image_count = 0  # Contador para guardar imágenes con nombres únicos\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Captura el frame actual\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Guardar la imagen original capturada\n",
    "    cv2.imwrite(f\"{output_dir}/original_{image_count}.jpg\", frame)\n",
    "\n",
    "    # Convertir la imagen a escala de grises\n",
    "    img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Redimensionar la imagen a 48x48 usando interpolación cúbica para mayor calidad\n",
    "    img_gray_resized = cv2.resize(img_gray, (48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Guardar la imagen redimensionada a 48x48 para inspección\n",
    "    cv2.imwrite(f\"{output_dir}/preprocessed_{image_count}.jpg\", img_gray_resized)\n",
    "\n",
    "    # Convertir la imagen redimensionada a tensor para el modelo\n",
    "    img_tensor = transform(Image.fromarray(img_gray_resized)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Realiza la predicción\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        emotion = emotion_map[predicted.item()]\n",
    "\n",
    "    # Muestra la predicción en el frame\n",
    "    cv2.putText(frame, f'Emocion: {emotion}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Muestra el frame con la predicc\n"
   ],
   "id": "c44db46f6f5520c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pv/hj1tkdtn4xd2lk842kp9xy7r0000gn/T/ipykernel_6605/439842556.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('modelo_emociones_imagen.pth', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m image_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# Contador para guardar imágenes con nombres únicos\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 44\u001B[0m     ret, frame \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()  \u001B[38;5;66;03m# Captura el frame actual\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T03:35:01.904817Z",
     "start_time": "2024-08-22T03:35:01.805422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Release resources\n",
    "cap.release()\n",
    "\n",
    "# Destroy all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "79d32ddb3b76f655",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e77cd8f976d1d71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
